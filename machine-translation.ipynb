{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12836544,"sourceType":"datasetVersion","datasetId":8118500}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 1: Setup dan Pemuatan Data\n\nKode ini berisi semua impor pustaka, konfigurasi dasar, dan kode untuk memuat serta membagi data terjemahan ke dalam set pelatihan, validasi, dan pengujian.","metadata":{}},{"cell_type":"code","source":"!pip install sacrebleu\nimport os, pathlib, random, math, time, subprocess, sys, json\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom collections import defaultdict\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom tqdm.notebook import tqdm\nimport sentencepiece as spm\nimport sacrebleu\n\n# Install sacrebleu jika belum terinstal\ntry:\n    import sacrebleu\nexcept ImportError:\n    !pip install sacrebleu\n    import sacrebleu\n\n# --- Reproducibility helper ---\ndef set_seed(seed: int = 42):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\nset_seed(42)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Menggunakan device: {DEVICE}\")\n\nWORKDIR = pathlib.Path(\"/kaggle/working\")\nWORKDIR.mkdir(parents=True, exist_ok=True)\n\n# ---------------------------------------------------------\n# Data: ManyThings loader\n# ---------------------------------------------------------\ndef load_translation_data():\n    possible_paths = [\n        \"/kaggle/input/translate-in/ind.txt\",\n        str(WORKDIR/\"ind.txt\"),\n        \"/kaggle/input/translate/ind.txt\",\n        \"/kaggle/input/ind.txt\"\n    ]\n    for path in possible_paths:\n        if pathlib.Path(path).exists():\n            print(f\"Menggunakan data dari: {path}\")\n            return pathlib.Path(path)\n    print(\"Warning: Tidak ada file data yang ditemukan. Pastikan data ada di salah satu path berikut.\")\n    return None\n\nDATA_PATH = load_translation_data()\nif not DATA_PATH:\n    raise FileNotFoundError(\"Dataset ind.txt tidak ditemukan di lokasi yang diharapkan.\")\n\nMAX_LEN = 50\nVAL_RATIO, TEST_RATIO = 0.1, 0.1\npairs = []\nwith open(DATA_PATH, encoding=\"utf-8\") as f:\n    for line in f:\n        parts = line.strip().split(\"\\t\")\n        if len(parts) < 2: continue\n        en, idn = parts[0].lower().strip(), parts[1].lower().strip()\n        if len(en.split()) > MAX_LEN or len(idn.split()) > MAX_LEN: continue\n        pairs.append((en, idn))\n\nrandom.shuffle(pairs)\nn_total = len(pairs)\nn_val, n_test = int(n_total*VAL_RATIO), int(n_total*TEST_RATIO)\ntest_pairs = pairs[:n_test]\nval_pairs = pairs[n_test:n_test+n_val]\ntrain_pairs= pairs[n_test+n_val:]\n\nprint(f\"[Data] Total={n_total} | Train={len(train_pairs)}, Valid={len(val_pairs)}, Test={len(test_pairs)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T06:56:06.613353Z","iopub.execute_input":"2025-08-24T06:56:06.613703Z","iopub.status.idle":"2025-08-24T06:56:17.644405Z","shell.execute_reply.started":"2025-08-24T06:56:06.613677Z","shell.execute_reply":"2025-08-24T06:56:17.643315Z"}},"outputs":[{"name":"stdout","text":"Collecting sacrebleu\n  Downloading sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting portalocker (from sacrebleu)\n  Downloading portalocker-3.2.0-py3-none-any.whl.metadata (8.7 kB)\nRequirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (2024.11.6)\nRequirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (0.9.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (1.26.4)\nRequirement already satisfied: colorama in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (0.4.6)\nRequirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (5.4.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->sacrebleu) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->sacrebleu) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->sacrebleu) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->sacrebleu) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->sacrebleu) (2024.2.0)\nDownloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading portalocker-3.2.0-py3-none-any.whl (22 kB)\nInstalling collected packages: portalocker, sacrebleu\nSuccessfully installed portalocker-3.2.0 sacrebleu-2.5.1\nMenggunakan device: cpu\nMenggunakan data dari: /kaggle/input/ind.txt\n[Data] Total=14881 | Train=11905, Valid=1488, Test=1488\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"Penjelasan:\n\nkode ini adalah bagian setup. Fungsinya adalah:\n\n- Mengimpor semua pustaka yang diperlukan.\n\n- Menentukan device yang akan digunakan (GPU atau CPU).\n\n- Membuat direktori kerja (/kaggle/working) untuk menyimpan hasil.\n\n- Memuat dataset dan membaginya menjadi tiga set (pelatihan, validasi, dan pengujian).","metadata":{}},{"cell_type":"markdown","source":"# 2: Definisi Model dan Fungsi Bantu\n\nKode ini berisi semua definisi model (Seq2Seq, Transformer) dan fungsi-fungsi bantu yang akan dipanggil selama proses pelatihan dan evaluasi.","metadata":{}},{"cell_type":"code","source":"# ---------------------------------------------------------\n# Helper Functions: SentencePiece + Fallback\n# ---------------------------------------------------------\ndef train_sentencepiece_model(corpus_path, model_prefix, vocab_size):\n    spm.SentencePieceTrainer.Train(\n        f\"--input={corpus_path} --model_prefix={model_prefix} \"\n        f\"--vocab_size={vocab_size} --character_coverage=1.0 \"\n        f\"--bos_id=1 --eos_id=2 --pad_id=3 --unk_id=0 --hard_vocab_limit=false\"\n    )\n\ndef create_tokenizers(train_pairs, vocab_size):\n    en_corp = WORKDIR / f\"train_text_v{vocab_size}.en\"\n    id_corp = WORKDIR / f\"train_text_v{vocab_size}.id\"\n    with open(en_corp, \"w\", encoding=\"utf-8\") as f:\n        for en,_ in train_pairs: f.write(en+\"\\n\")\n    with open(id_corp, \"w\", encoding=\"utf-8\") as f:\n        for _,idn in train_pairs: f.write(idn+\"\\n\")\n    en_prefix = WORKDIR / f\"en_spm_v{vocab_size}\"\n    id_prefix = WORKDIR / f\"id_spm_v{vocab_size}\"\n    try:\n        train_sentencepiece_model(en_corp, str(en_prefix), vocab_size)\n        train_sentencepiece_model(id_corp, str(id_prefix), vocab_size)\n        sp_en, sp_id = spm.SentencePieceProcessor(), spm.SentencePieceProcessor()\n        sp_en.load(str(en_prefix) + \".model\")\n        sp_id.load(str(id_prefix) + \".model\")\n        print(f\"[SPM] Model saved: {en_prefix}.model and {id_prefix}.model\")\n        return sp_en, sp_id\n    except Exception as e:\n        print(\"SentencePiece failed:\", e)\n        raise\n\n# ---------------------------------------------------------\n# Dataset & DataLoader\n# ---------------------------------------------------------\nclass TranslationDataset(Dataset):\n    def __init__(self, pairs, sp_src, sp_tgt, max_len=MAX_LEN):\n        self.data = pairs\n        self.sp_src, self.sp_tgt = sp_src, sp_tgt\n        self.max_len = max_len\n    def __len__(self):\n        return len(self.data)\n    def __getitem__(self, idx):\n        en, idn = self.data[idx]\n        src_ids = [self.sp_src.bos_id()] + self.sp_src.encode(en) + [self.sp_src.eos_id()]\n        tgt_ids = [self.sp_tgt.bos_id()] + self.sp_tgt.encode(idn) + [self.sp_tgt.eos_id()]\n        src = torch.tensor(src_ids[:self.max_len], dtype=torch.long)\n        tgt = torch.tensor(tgt_ids[:self.max_len], dtype=torch.long)\n        return src, tgt\n\ndef collate_batch(batch, pad_src, pad_tgt):\n    src_batch, tgt_batch = zip(*batch)\n    src_lens = [len(s) for s in src_batch]\n    src_pad = nn.utils.rnn.pad_sequence(src_batch, padding_value=pad_src, batch_first=True)\n    tgt_pad = nn.utils.rnn.pad_sequence(tgt_batch, padding_value=pad_tgt, batch_first=True)\n    return src_pad, tgt_pad, src_lens, None\n\n# ---------------------------------------------------------\n# Seq2Seq Model Definitions (Encoder, Attention, Decoder)\n# ---------------------------------------------------------\nclass EncoderRNN(nn.Module):\n    def __init__(self, input_dim, emb_dim, hid_dim, n_layers=1, dropout=0.1, bidirectional=True, pad_idx=3):\n        super().__init__()\n        self.embedding = nn.Embedding(input_dim, emb_dim, padding_idx=pad_idx)\n        self.rnn = nn.GRU(emb_dim, hid_dim, n_layers, batch_first=True, bidirectional=bidirectional, dropout=dropout if n_layers>1 else 0)\n        self.dropout = nn.Dropout(dropout)\n        self.bi = 2 if bidirectional else 1\n        self.hid_dim = hid_dim\n        self.n_layers = n_layers\n    def forward(self, src, src_lens):\n        embedded = self.dropout(self.embedding(src))\n        packed = nn.utils.rnn.pack_padded_sequence(embedded, src_lens, batch_first=True, enforce_sorted=False)\n        outputs, hidden = self.rnn(packed)\n        outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs, batch_first=True)\n        return outputs, hidden\nclass AdditiveAttention(nn.Module):\n    def __init__(self, enc_dim, dec_dim):\n        super().__init__()\n        self.W = nn.Linear(enc_dim + dec_dim, dec_dim)\n        self.v = nn.Linear(dec_dim, 1, bias=False)\n    def forward(self, dec_hidden, enc_outputs, src_mask):\n        T = enc_outputs.size(1)\n        dec_rep = dec_hidden.unsqueeze(1).repeat(1, T, 1)\n        energy = torch.tanh(self.W(torch.cat([dec_rep, enc_outputs], dim=-1)))\n        scores = self.v(energy).squeeze(-1)\n        scores = scores.masked_fill(src_mask==0, -1e9)\n        attn = torch.softmax(scores, dim=-1)\n        ctx = torch.bmm(attn.unsqueeze(1), enc_outputs).squeeze(1)\n        return ctx, attn\nclass DecoderRNN(nn.Module):\n    def __init__(self, output_dim, emb_dim, enc_hid, dec_hid, dropout=0.1, pad_idx=3):\n        super().__init__()\n        self.embedding = nn.Embedding(output_dim, emb_dim, padding_idx=pad_idx)\n        self.rnn = nn.GRU(emb_dim + enc_hid, dec_hid, batch_first=True)\n        self.fc = nn.Linear(dec_hid + enc_hid, output_dim)\n        self.dropout = nn.Dropout(dropout)\n        self.attn = AdditiveAttention(enc_hid, dec_hid)\n    def forward(self, y_prev, hidden, enc_outputs, src_mask):\n        emb = self.dropout(self.embedding(y_prev)).unsqueeze(1)\n        dec_hidden = hidden[-1]\n        ctx, attn = self.attn(dec_hidden, enc_outputs, src_mask)\n        rnn_input = torch.cat([(emb), ctx.unsqueeze(1)], dim=-1)\n        out, hidden = self.rnn(rnn_input, hidden)\n        logits = self.fc(torch.cat([out.squeeze(1), ctx], dim=-1))\n        return logits, hidden, attn\nclass Seq2SeqAttn(nn.Module):\n    def __init__(self, encoder, decoder, enc_bi=True, pad_src=3):\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        self.enc_bi = enc_bi\n        self.pad_src = pad_src\n    def make_src_mask(self, src):\n        return (src != self.pad_src).to(src.device)\n    def forward(self, src, src_lens, tgt):\n        enc_outputs, enc_hidden = self.encoder(src, src_lens)\n        if self.enc_bi:\n            enc_hidden = enc_hidden.view(self.encoder.n_layers, 2, enc_hidden.size(1), enc_hidden.size(2)).sum(1)\n        src_mask = self.make_src_mask(src)\n        B, Tt = tgt.size()\n        hidden = enc_hidden\n        logits_all = []\n        y = tgt[:,0]\n        for t in range(1, Tt):\n            logits, hidden, _ = self.decoder(y, hidden, enc_outputs, src_mask)\n            logits_all.append(logits.unsqueeze(1))\n            y = tgt[:,t]\n        return torch.cat(logits_all, dim=1)\n    @torch.no_grad()\n    def translate(self, src, src_lens, max_len=80, bos_tgt=1, eos_tgt=2):\n        self.eval()\n        enc_outputs, enc_hidden = self.encoder(src, src_lens)\n        if self.enc_bi:\n            enc_hidden = enc_hidden.view(self.encoder.n_layers, 2, enc_hidden.size(1), enc_hidden.size(2)).sum(1)\n        src_mask = self.make_src_mask(src)\n        B = src.size(0)\n        hidden = enc_hidden\n        y = torch.full((B,), bos_tgt, dtype=torch.long, device=src.device)\n        out_ids = [y.clone()]\n        for _ in range(max_len):\n            logits, hidden, _ = self.decoder(y, hidden, enc_outputs, src_mask)\n            y = logits.argmax(dim=-1)\n            out_ids.append(y.clone())\n            if (y == eos_tgt).all(): break\n        out = torch.stack(out_ids, dim=1)\n        return out\n\n# ---------------------------------------------------------\n# Transformer Model Definitions\n# ---------------------------------------------------------\nclass LabelSmoothingLoss(nn.Module):\n    def __init__(self, size, smoothing=0.1, pad_idx=3):\n        super().__init__()\n        self.criterion = nn.KLDivLoss(reduction='batchmean')\n        self.smoothing = smoothing\n        self.size = size\n        self.pad_idx = pad_idx\n    def forward(self, x, target):\n        true_dist = torch.zeros_like(x)\n        true_dist.fill_(self.smoothing / (self.size - 2))\n        true_dist.scatter_(1, target.unsqueeze(1), 1.0 - self.smoothing)\n        true_dist[:, self.pad_idx] = 0\n        mask = (target == self.pad_idx).unsqueeze(1).float()\n        true_dist = true_dist * (1 - mask)\n        return self.criterion(F.log_softmax(x, dim=1), true_dist)\nclass TransformerMT(nn.Module):\n    def __init__(self, src_vocab, tgt_vocab, d_model=256, nhead=8, num_layers=3, dim_ff=512, dropout=0.1, pad_src=3, pad_tgt=3):\n        super().__init__()\n        self.src_emb = nn.Embedding(src_vocab, d_model, padding_idx=pad_src)\n        self.tgt_emb = nn.Embedding(tgt_vocab, d_model, padding_idx=pad_tgt)\n        self.pos_enc_src = nn.Parameter(torch.randn(1, 1024, d_model) * 0.01)\n        self.pos_enc_tgt = nn.Parameter(torch.randn(1, 1024, d_model) * 0.01)\n        self.tr = nn.Transformer(d_model=d_model, nhead=nhead, num_encoder_layers=num_layers, num_decoder_layers=num_layers, dim_feedforward=dim_ff, dropout=dropout, batch_first=True)\n        self.fc = nn.Linear(d_model, tgt_vocab)\n        self.pad_src = pad_src\n        self.pad_tgt = pad_tgt\n        self.d_model = d_model\n    def src_mask(self, src):\n        return (src != self.pad_src)\n    def tgt_mask(self, size):\n        mask = torch.triu(torch.ones(size, size, device=DEVICE), diagonal=1).bool()\n        return mask\n    def forward(self, src, tgt_in):\n        B, Ts = src.size()\n        _, Tt = tgt_in.size()\n        src_e = self.src_emb(src) * math.sqrt(self.d_model) + self.pos_enc_src[:, :Ts, :]\n        tgt_e = self.tgt_emb(tgt_in) * math.sqrt(self.d_model) + self.pos_enc_tgt[:, :Tt, :]\n        src_key_padding_mask = ~self.src_mask(src)\n        tgt_key_padding_mask = (tgt_in == self.pad_tgt)\n        causal_mask = self.tgt_mask(Tt)\n        out = self.tr(src_e, tgt_e, tgt_mask=causal_mask, src_key_padding_mask=src_key_padding_mask, tgt_key_padding_mask=tgt_key_padding_mask, memory_key_padding_mask=src_key_padding_mask)\n        return self.fc(out)\n    @torch.no_grad()\n    def translate_greedy(self, src, max_len=80, bos_tgt=1, eos_tgt=2):\n        self.eval()\n        B, Ts = src.size()\n        src_e = self.src_emb(src) * math.sqrt(self.d_model) + self.pos_enc_src[:, :Ts, :]\n        src_key_padding_mask = (src == self.pad_src)\n        memory = self.tr.encoder(src_e, src_key_padding_mask=src_key_padding_mask)\n        ys = torch.full((B,1), bos_tgt, dtype=torch.long, device=src.device)\n        for _ in range(max_len):\n            Tt = ys.size(1)\n            tgt_e = self.tgt_emb(ys) * math.sqrt(self.d_model) + self.pos_enc_tgt[:, :Tt, :]\n            causal_mask = self.tgt_mask(Tt)\n            out = self.tr.decoder(tgt_e, memory, tgt_mask=causal_mask, tgt_key_padding_mask=(ys==self.pad_tgt), memory_key_padding_mask=src_key_padding_mask)\n            logits = self.fc(out[:,-1,:])\n            next_tok = logits.argmax(dim=-1, keepdim=True)\n            ys = torch.cat([ys, next_tok], dim=1)\n            if (next_tok.squeeze(-1) == eos_tgt).all(): break\n        return ys\n    @torch.no_grad()\n    def translate_beam(self, src, max_len=80, bos_tgt=1, eos_tgt=2, beam=4):\n        self.eval()\n        assert src.size(0) == 1, \"Beam search implemented for batch=1 for simplicity\"\n        Ts = src.size(1)\n        src_e = self.src_emb(src) * math.sqrt(self.d_model) + self.pos_enc_src[:, :Ts, :]\n        src_key_padding_mask = (src == self.pad_src)\n        memory = self.tr.encoder(src_e, src_key_padding_mask=src_key_padding_mask)\n        beams = [(torch.tensor([[bos_tgt]], device=src.device), 0.0)] # (seq, logprob)\n        for _ in range(max_len):\n            new_beams = []\n            for seq, lp in beams:\n                if seq[0,-1].item() == eos_tgt:\n                    new_beams.append((seq, lp))\n                    continue\n                Tt = seq.size(1)\n                tgt_e = self.tgt_emb(seq) * math.sqrt(self.d_model) + self.pos_enc_tgt[:, :Tt, :]\n                causal_mask = self.tgt_mask(Tt)\n                out = self.tr.decoder(tgt_e, memory, tgt_mask=causal_mask, tgt_key_padding_mask=(seq==self.pad_tgt), memory_key_padding_mask=src_key_padding_mask)\n                logits = self.fc(out[:,-1,:])\n                logp = F.log_softmax(logits, dim=-1).squeeze(0)\n                topk = torch.topk(logp, beam)\n                for token, token_lp in zip(topk.indices.tolist(), topk.values.tolist()):\n                    new_seq = torch.cat([seq, torch.tensor([[token]], device=src.device)], dim=1)\n                    new_beams.append((new_seq, lp + token_lp))\n            new_beams.sort(key=lambda x: x[1], reverse=True)\n            beams = new_beams[:beam]\n            if all(seq[0,-1].item() == eos_tgt for seq,_ in beams): break\n        best_seq = max(beams, key=lambda x: x[1])[0]\n        return best_seq\n\n# ---------------------------------------------------------\n# Training & Evaluation Functions\n# ---------------------------------------------------------\ndef epoch_train_transformer(model, dl, optimizer, criterion, scheduler=None, grad_clip=1.0):\n    model.train()\n    total_loss = 0.0\n    for src, tgt, src_lens, _ in tqdm(dl, desc=\"Transformer Training\"):\n        src, tgt = src.to(DEVICE), tgt.to(DEVICE)\n        optimizer.zero_grad()\n        logits = model(src, tgt[:, :-1])\n        loss = criterion(logits.reshape(-1, logits.size(-1)), tgt[:, 1:].reshape(-1))\n        loss.backward()\n        nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n        optimizer.step()\n        if scheduler: scheduler.step()\n        total_loss += loss.item()\n    return total_loss / len(dl)\n\ndef epoch_train_seq2seq(model, dl, optimizer, criterion, grad_clip=1.0):\n    model.train()\n    total_loss = 0.0\n    for src, tgt, src_lens, _ in tqdm(dl, desc=\"RNN+Attn Training\"):\n        src, tgt = src.to(DEVICE), tgt.to(DEVICE)\n        optimizer.zero_grad()\n        logits = model(src, src_lens, tgt)\n        loss = criterion(logits.reshape(-1, logits.size(-1)), tgt[:,1:].reshape(-1))\n        loss.backward()\n        nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n        optimizer.step()\n        total_loss += loss.item()\n    return total_loss / len(dl)\n\ndef decode_transformer(model, src, sp_tgt, max_len=80, mode='greedy', beam=4):\n    if mode == 'beam': return model.translate_beam(src.to(DEVICE), max_len=max_len, bos_tgt=sp_tgt.bos_id(), eos_tgt=sp_tgt.eos_id(), beam=beam).cpu()\n    return model.translate_greedy(src.to(DEVICE), max_len=max_len, bos_tgt=sp_tgt.bos_id(), eos_tgt=sp_tgt.eos_id()).cpu()\n\ndef decode_seq2seq(model, src, src_lens, sp_tgt, max_len=80):\n    return model.translate(src.to(DEVICE), src_lens, max_len=max_len, bos_tgt=sp_tgt.bos_id(), eos_tgt=sp_tgt.eos_id()).cpu()\n\ndef eval_bleu_chrF_with_model(model, dl, decode_fn, sp_src, sp_tgt, max_len=80, n_samples_preview=5, **decode_kwargs):\n    refs, hyps, previews = [], [], []\n    model.eval()\n    with torch.no_grad():\n        for i,(src, tgt, src_lens, _) in tqdm(enumerate(dl), total=len(dl), desc=\"Evaluating\"):\n            out_ids = decode_fn(model, src, src_lens, sp_tgt, max_len=max_len) if decode_fn == decode_seq2seq else decode_fn(model, src, sp_tgt, max_len=max_len, **decode_kwargs)\n            pred_ids = out_ids[0].tolist()\n            if len(pred_ids) > 0 and pred_ids[0] == sp_tgt.bos_id(): pred_ids = pred_ids[1:]\n            if sp_tgt.eos_id() in pred_ids: pred_ids = pred_ids[:pred_ids.index(sp_tgt.eos_id())]\n            hyp = sp_tgt.decode(pred_ids)\n            ref_ids = tgt[0].tolist()\n            if len(ref_ids) > 0 and ref_ids[0] == sp_tgt.bos_id(): ref_ids = ref_ids[1:]\n            if sp_tgt.eos_id() in ref_ids: ref_ids = ref_ids[:ref_ids.index(sp_tgt.eos_id())]\n            ref = sp_tgt.decode(ref_ids)\n            hyps.append(hyp); refs.append([ref])\n            if i < n_samples_preview:\n                src_ids = src[0].tolist()\n                if len(src_ids) > 0 and src_ids[0] == sp_src.bos_id(): src_ids = src_ids[1:]\n                if sp_src.eos_id() in src_ids: src_ids = src_ids[:src_ids.index(sp_src.eos_id())]\n                src_txt = sp_src.decode(src_ids)\n                previews.append((src_txt, hyp, ref))\n    bleu = sacrebleu.corpus_bleu(hyps, refs).score\n    chrf = sacrebleu.corpus_chrf(hyps, refs).score\n    return bleu, chrf, previews\n\ndef analyze_translation_errors(previews, save_path):\n    analyses = [{'src': s, 'hyp': h, 'ref': r, 'src_len': len(s.split()), 'hyp_len': len(h.split()), 'ref_len': len(r.split()),\n                 'error_type': \"Under-translation\" if len(h.split()) < len(r.split())*0.7 else \"Over-translation\" if len(h.split()) > len(r.split())*1.3 else \"Normal\"}\n                for s,h,r in previews]\n    pd.DataFrame(analyses).to_csv(save_path, index=False)\n    return analyses\n\ndef plot_training_curves(history, model_name, save_path):\n    epochs = range(1, len(history['train_loss']) + 1)\n    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n    axes[0].plot(epochs, history['train_loss']); axes[0].set_title(f'{model_name} - Training Loss'); axes[0].set_xlabel('Epoch'); axes[0].set_ylabel('Loss'); axes[0].grid(True)\n    axes[1].plot(epochs, history['val_bleu']); axes[1].set_title(f'{model_name} - Validation BLEU'); axes[1].set_xlabel('Epoch'); axes[1].set_ylabel('BLEU'); axes[1].grid(True)\n    axes[2].plot(epochs, history['val_chrf']); axes[2].set_title(f'{model_name} - Validation chrF'); axes[2].set_xlabel('Epoch'); axes[2].set_ylabel('chrF'); axes[2].grid(True)\n    plt.tight_layout(); plt.savefig(save_path, dpi=150, bbox_inches='tight'); plt.close()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T06:56:17.646102Z","iopub.execute_input":"2025-08-24T06:56:17.646496Z","iopub.status.idle":"2025-08-24T06:56:17.707029Z","shell.execute_reply.started":"2025-08-24T06:56:17.646469Z","shell.execute_reply":"2025-08-24T06:56:17.706011Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"Penjelasan:\n\node ini adalah inti dari proyek . Isinya adalah:\n\n- Definisi kelas untuk model RNN+Attention dan Transformer.\n\n- Fungsi-fungsi untuk melatih model per epoch.\n\n- Fungsi-fungsi untuk menerjemahkan teks dengan model yang sudah dilatih.\n\n- Fungsi untuk menghitung metrik evaluasi seperti BLEU dan chrF.","metadata":{}},{"cell_type":"markdown","source":"# 3: Fungsi Eksperimen Utama\n\nKode ini berisi fungsi utama (run_experiment) yang menjalankan satu eksperimen lengkap untuk satu ukuran vocabulary tertentu, mulai dari melatih model hingga menyimpan hasilnya.","metadata":{}},{"cell_type":"code","source":"# ---------------------------------------------------------\n# MAIN EXPERIMENT: ABLATION + TRAINING\n# ---------------------------------------------------------\ndef run_experiment(vocab_size, epochs_tr=15, epochs_rnn=15, use_beam_eval=True, beam_size=4):\n    print(f\"\\n{'='*60}\\nEXPERIMENT: VOCAB_SIZE = {vocab_size}\\n{'='*60}\")\n    print(\"[1/6] Membuat tokenizer…\")\n    sp_en, sp_id = create_tokenizers(train_pairs, vocab_size)\n    SRC_VOCAB, TGT_VOCAB = sp_en.get_piece_size(), sp_id.get_piece_size()\n    PAD_SRC, EOS_SRC = sp_en.pad_id(), sp_en.eos_id()\n    PAD_TGT, BOS_TGT, EOS_TGT = sp_id.pad_id(), sp_id.bos_id(), sp_id.eos_id()\n    print(f\"[SPM] EN vocab: {SRC_VOCAB} | ID vocab: {TGT_VOCAB}\")\n    \n    print(\"[2/6] Membuat datasets & loaders…\")\n    train_ds = TranslationDataset(train_pairs, sp_en, sp_id)\n    val_ds = TranslationDataset(val_pairs, sp_en, sp_id)\n    test_ds = TranslationDataset(test_pairs, sp_en, sp_id)\n    def collate_fn(batch): return collate_batch(batch, PAD_SRC, PAD_TGT)\n    BATCH_SIZE = 32\n    train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n    val_dl = DataLoader(val_ds, batch_size=1, shuffle=False, collate_fn=collate_fn)\n    test_dl = DataLoader(test_ds, batch_size=1, shuffle=False, collate_fn=collate_fn)\n\n    print(f\"[3/6] Melatih Transformer (vocab={vocab_size})…\")\n    d_model, nhead, num_layers, dim_ff, dropout = 128, 4, 2, 256, 0.1\n    transformer = TransformerMT(SRC_VOCAB, TGT_VOCAB, d_model, nhead, num_layers, dim_ff, dropout, PAD_SRC, PAD_TGT).to(DEVICE)\n    optim_tr = optim.Adam(transformer.parameters(), lr=1e-4, betas=(0.9, 0.98), eps=1e-9)\n    lr_scheduler_tr = optim.lr_scheduler.ReduceLROnPlateau(optim_tr, mode='max', patience=2, factor=0.5, verbose=True)\n    criterion_tr = LabelSmoothingLoss(TGT_VOCAB, smoothing=0.1, pad_idx=PAD_TGT)\n    tr_history = {'train_loss': [], 'val_bleu': [], 'val_chrf': []}; best_tr_bleu = 0\n    for ep in range(1, epochs_tr + 1):\n        loss = epoch_train_transformer(transformer, train_dl, optim_tr, criterion_tr)\n        bleu, chrf, _ = eval_bleu_chrF_with_model(transformer, val_dl, decode_transformer, sp_en, sp_id, max_len=80, mode=('beam' if use_beam_eval else 'greedy'), beam=beam_size)\n        tr_history['train_loss'].append(loss); tr_history['val_bleu'].append(bleu); tr_history['val_chrf'].append(chrf)\n        lr_scheduler_tr.step(bleu)\n        if bleu > best_tr_bleu:\n            best_tr_bleu = bleu\n            torch.save(transformer.state_dict(), WORKDIR/f\"best_transformer_v{vocab_size}.pt\")\n        print(f\"[Transformer][Epoch {ep:2d}] loss={loss:.4f} | valBLEU={bleu:.2f} | valchrF={chrf:.2f}\")\n\n    print(f\"[4/6] Melatih RNN+Attention (vocab={vocab_size})…\")\n    emb_enc, emb_dec, hid, layers, dropout = 128, 128, 128, 1, 0.1\n    encoder = EncoderRNN(SRC_VOCAB, emb_enc, hid, layers, dropout, True, PAD_SRC)\n    decoder = DecoderRNN(TGT_VOCAB, emb_dec, hid*2, hid, dropout, PAD_TGT)\n    seq2seq = Seq2SeqAttn(encoder, decoder, enc_bi=True, pad_src=PAD_SRC).to(DEVICE)\n    optim_rnn = optim.Adam(seq2seq.parameters(), lr=3e-4)\n    lr_scheduler_rnn = optim.lr_scheduler.ReduceLROnPlateau(optim_rnn, mode='max', patience=2, factor=0.5, verbose=True)\n    criterion_rnn = nn.CrossEntropyLoss(ignore_index=PAD_TGT)\n    rnn_history = {'train_loss': [], 'val_bleu': [], 'val_chrf': []}; best_rnn_bleu = 0\n    for ep in range(1, epochs_rnn + 1):\n        loss = epoch_train_seq2seq(seq2seq, train_dl, optim_rnn, criterion_rnn)\n        bleu, chrf, _ = eval_bleu_chrF_with_model(seq2seq, val_dl, decode_seq2seq, sp_en, sp_id, max_len=80)\n        rnn_history['train_loss'].append(loss); rnn_history['val_bleu'].append(bleu); rnn_history['val_chrf'].append(chrf)\n        lr_scheduler_rnn.step(bleu)\n        if bleu > best_rnn_bleu:\n            best_rnn_bleu = bleu\n            torch.save(seq2seq.state_dict(), WORKDIR/f\"best_rnn_v{vocab_size}.pt\")\n        print(f\"[RNN+Attn][Epoch {ep:2d}] loss={loss:.4f} | valBLEU={bleu:.2f} | valchrF={chrf:.2f}\")\n\n    print(f\"[5/6] Evaluasi akhir (vocab={vocab_size})…\")\n    transformer.load_state_dict(torch.load(WORKDIR/f\"best_transformer_v{vocab_size}.pt\"))\n    seq2seq.load_state_dict(torch.load(WORKDIR/f\"best_rnn_v{vocab_size}.pt\"))\n    tr_bleu, tr_chrf, tr_prev = eval_bleu_chrF_with_model(transformer, test_dl, decode_transformer, sp_en, sp_id, max_len=80, mode=('beam' if use_beam_eval else 'greedy'), beam=beam_size)\n    rn_bleu, rn_chrf, rn_prev = eval_bleu_chrF_with_model(seq2seq, test_dl, decode_seq2seq, sp_en, sp_id, max_len=80)\n    print(f\"\\n[FINAL] Transformer : BLEU={tr_bleu:.2f} | chrF={tr_chrf:.2f}\")\n    print(f\"[FINAL] RNN+Attention: BLEU={rn_bleu:.2f} | chrF={rn_chrf:.2f}\")\n    \n    print(f\"[6/6] Menyimpan hasil (vocab={vocab_size})…\")\n    # Perbaikan: Menggunakan try-except untuk mencegah kegagalan saat menyimpan file\n    try:\n        tr_prev_df = pd.DataFrame(tr_prev, columns=['src', 'hyp', 'ref'])\n        tr_prev_df.to_csv(WORKDIR/f\"preview_transformer_v{vocab_size}.csv\", index=False)\n        rn_prev_df = pd.DataFrame(rn_prev, columns=['src', 'hyp', 'ref'])\n        rn_prev_df.to_csv(WORKDIR/f\"preview_rnn_v{vocab_size}.csv\", index=False)\n        plot_training_curves(tr_history, f\"Transformer_v{vocab_size}\", WORKDIR/f\"transformer_curves_v{vocab_size}.png\")\n        plot_training_curves(rnn_history, f\"RNN+Attention_v{vocab_size}\", WORKDIR/f\"rnn_curves_v{vocab_size}.png\")\n        analyze_translation_errors(tr_prev, WORKDIR/f\"error_analysis_transformer_v{vocab_size}.csv\")\n        analyze_translation_errors(rn_prev, WORKDIR/f\"error_analysis_rnn_v{vocab_size}.csv\")\n    except Exception as e:\n        print(f\"Peringatan: Gagal menyimpan file CSV. Error: {e}\")\n        \n    return {'vocab_size': vocab_size, 'transformer': {'bleu': tr_bleu, 'chrf': tr_chrf, 'best_val_bleu': best_tr_bleu}, 'rnn': {'bleu': rn_bleu, 'chrf': rn_chrf, 'best_val_bleu': best_rnn_bleu}, 'tr_history': tr_history, 'rnn_history': rnn_history}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T06:56:17.708109Z","iopub.execute_input":"2025-08-24T06:56:17.708420Z","iopub.status.idle":"2025-08-24T06:56:17.736794Z","shell.execute_reply.started":"2025-08-24T06:56:17.708394Z","shell.execute_reply":"2025-08-24T06:56:17.735617Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"Penjelasan:\n\nkode ini mendefinisikan fungsi run_experiment. Fungsi ini sangat penting karena:\n\n- Ini adalah \"mesin\" yang menjalankan seluruh eksperimen untuk satu set parameter (misalnya, vocab_size=2000).\n\n- Kode ini sudah diperbaiki dengan menambahkan blok try-except saat menyimpan file CSV. Ini memastikan bahwa jika ada masalah saat menyimpan hasil, program tidak akan crash dan masih bisa menyelesaikan eksperimen.","metadata":{}},{"cell_type":"markdown","source":"#  4: Menjalankan Studi Ablasi dan Menampilkan Hasil\nKode ini memanggil fungsi-fungsi dari bagian sebelumnya untuk menjalankan studi ablasi secara keseluruhan, mengumpulkan semua hasil, dan menampilkan ringkasan akhir.","metadata":{}},{"cell_type":"code","source":"def run_ablation_study(vocab_sizes=(2000, 4000), epochs_tr=15, epochs_rnn=15, use_beam_eval=True, beam_size=4):\n    print(\"\\n\" + \"=\"*80)\n    print(\"MEMULAI STUDI ABLASI: PERBANDINGAN UKURAN VOCABULARY\")\n    print(\"=\"*80)\n    all_results = {}\n    for vocab_size in vocab_sizes:\n        start_time = time.time()\n        result = run_experiment(vocab_size, epochs_tr=epochs_tr, epochs_rnn=epochs_rnn, use_beam_eval=use_beam_eval, beam_size=beam_size)\n        end_time = time.time()\n        result['training_time'] = end_time - start_time\n        all_results[vocab_size] = result\n        print(f\"\\n[VOCAB={vocab_size}] Pelatihan selesai dalam {(end_time-start_time)/60:.1f} menit\")\n    return all_results\n\ndef create_ablation_summary(all_results):\n    print(\"\\n\" + \"=\"*80)\n    print(\"RINGKASAN STUDI ABLASI\")\n    print(\"=\"*80)\n    summary_data = [{'vocab_size': v, 'tr_bleu': r['transformer']['bleu'], 'tr_chrf': r['transformer']['chrf'],\n                     'rnn_bleu': r['rnn']['bleu'], 'rnn_chrf': r['rnn']['chrf'], 'training_time_min': r['training_time'] / 60}\n                    for v, r in all_results.items()]\n    summary_df = pd.DataFrame(summary_data)\n    summary_df.to_csv(WORKDIR/\"ablation_summary.csv\", index=False)\n    print(\"\\nHASIL ABLASI:\\n\" + summary_df.to_string(index=False, float_format='%.2f'))\n    best_tr_idx = summary_df['tr_bleu'].idxmax(); best_rnn_idx = summary_df['rnn_bleu'].idxmax()\n    print(f\"\\nKONFIGURASI TERBAIK:\\nTransformer: vocab={summary_df.loc[best_tr_idx, 'vocab_size']:.0f}, BLEU={summary_df.loc[best_tr_idx, 'tr_bleu']:.2f}\\nRNN+Attn : vocab={summary_df.loc[best_rnn_idx, 'vocab_size']:.0f}, BLEU={summary_df.loc[best_rnn_idx, 'rnn_bleu']:.2f}\")\n\n    fig, axes = plt.subplots(2, 2, figsize=(12, 8)); vocab_sizes = summary_df['vocab_size'].values\n    axes[0,0].plot(vocab_sizes, summary_df['tr_bleu'], 'o-', label='Transformer'); axes[0,0].plot(vocab_sizes, summary_df['rnn_bleu'], 's-', label='RNN+Attention'); axes[0,0].set_xlabel('Ukuran Vocabulary'); axes[0,0].set_ylabel('Skor BLEU'); axes[0,0].set_title('Test BLEU vs Ukuran Vocab'); axes[0,0].legend(); axes[0,0].grid(True, alpha=0.3)\n    axes[0,1].plot(vocab_sizes, summary_df['tr_chrf'], 'o-', label='Transformer'); axes[0,1].plot(vocab_sizes, summary_df['rnn_chrf'], 's-', label='RNN+Attention'); axes[0,1].set_xlabel('Ukuran Vocabulary'); axes[0,1].set_ylabel('Skor chrF'); axes[0,1].set_title('Test chrF vs Ukuran Vocab'); axes[0,1].legend(); axes[0,1].grid(True, alpha=0.3)\n    axes[1,0].bar(vocab_sizes - 50, summary_df['training_time_min'], width=100, alpha=0.7); axes[1,0].set_xlabel('Ukuran Vocabulary'); axes[1,0].set_ylabel('Waktu Pelatihan (menit)'); axes[1,0].set_title('Waktu Pelatihan vs Ukuran Vocab'); axes[1,0].grid(True, alpha=0.3)\n    x_pos = np.arange(len(vocab_sizes)); width = 0.35\n    axes[1,1].bar(x_pos - width/2, summary_df['tr_bleu'], width, label='Transformer'); axes[1,1].bar(x_pos + width/2, summary_df['rnn_bleu'], width, label='RNN+Attention'); axes[1,1].set_xlabel('Ukuran Vocabulary'); axes[1,1].set_ylabel('Skor BLEU'); axes[1,1].set_title('Perbandingan Model (BLEU)'); axes[1,1].set_xticks(x_pos); axes[1,1].set_xticklabels(vocab_sizes); axes[1,1].legend(); axes[1,1].grid(True, alpha=0.3)\n    plt.tight_layout(); plt.savefig(WORKDIR/\"ablation_study_results.png\", dpi=150, bbox_inches='tight'); plt.close()\n    return summary_df\n\nif __name__ == '__main__':\n    all_results = run_ablation_study(\n        vocab_sizes=(2000, 4000),\n        epochs_tr=10,\n        epochs_rnn=10,\n        use_beam_eval=True,\n        beam_size=4\n    )\n    summary_df = create_ablation_summary(all_results)\n    \n    print(\"\\n\" + \"=\"*80)\n    print(\"CONTOH TERJEMAHAN DARI HASIL TERBAIK\")\n    print(\"=\"*80)\n    try:\n        best_vocab = int(summary_df.loc[summary_df['tr_bleu'].idxmax(), 'vocab_size'])\n        tr_df = pd.read_csv(WORKDIR/f\"preview_transformer_v{best_vocab}.csv\")\n        rnn_df = pd.read_csv(WORKDIR/f\"preview_rnn_v{best_vocab}.csv\")\n        print(\"\\n--- Prediksi Transformer ---\")\n        print(tr_df.head(5))\n        print(\"\\n--- Prediksi RNN+Attention ---\")\n        print(rnn_df.head(5))\n    except Exception as e:\n        print(f\"Gagal memuat file CSV: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T06:56:17.738635Z","iopub.execute_input":"2025-08-24T06:56:17.738979Z","iopub.status.idle":"2025-08-24T08:32:45.220779Z","shell.execute_reply.started":"2025-08-24T06:56:17.738945Z","shell.execute_reply":"2025-08-24T08:32:45.219930Z"}},"outputs":[{"name":"stdout","text":"\n================================================================================\nMEMULAI STUDI ABLASI: PERBANDINGAN UKURAN VOCABULARY\n================================================================================\n\n============================================================\nEXPERIMENT: VOCAB_SIZE = 2000\n============================================================\n[1/6] Membuat tokenizer…\n","output_type":"stream"},{"name":"stderr","text":"sentencepiece_trainer.cc(178) LOG(INFO) Running command: --input=/kaggle/working/train_text_v2000.en --model_prefix=/kaggle/working/en_spm_v2000 --vocab_size=2000 --character_coverage=1.0 --bos_id=1 --eos_id=2 --pad_id=3 --unk_id=0 --hard_vocab_limit=false\nsentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \ntrainer_spec {\n  input: /kaggle/working/train_text_v2000.en\n  input_format: \n  model_prefix: /kaggle/working/en_spm_v2000\n  model_type: UNIGRAM\n  vocab_size: 2000\n  self_test_sample_size: 0\n  character_coverage: 1\n  input_sentence_size: 0\n  shuffle_input_sentence: 1\n  seed_sentencepiece_size: 1000000\n  shrinking_factor: 0.75\n  max_sentence_length: 4192\n  num_threads: 16\n  num_sub_iterations: 2\n  max_sentencepiece_length: 16\n  split_by_unicode_script: 1\n  split_by_number: 1\n  split_by_whitespace: 1\n  split_digits: 0\n  pretokenization_delimiter: \n  treat_whitespace_as_suffix: 0\n  allow_whitespace_only_pieces: 0\n  required_chars: \n  byte_fallback: 0\n  vocabulary_output_piece_score: 1\n  train_extremely_large_corpus: 0\n  seed_sentencepieces_file: \n  hard_vocab_limit: 0\n  use_all_vocab: 0\n  unk_id: 0\n  bos_id: 1\n  eos_id: 2\n  pad_id: 3\n  unk_piece: <unk>\n  bos_piece: <s>\n  eos_piece: </s>\n  pad_piece: <pad>\n  unk_surface:  ⁇ \n  enable_differential_privacy: 0\n  differential_privacy_noise_level: 0\n  differential_privacy_clipping_threshold: 0\n}\nnormalizer_spec {\n  name: nmt_nfkc\n  add_dummy_prefix: 1\n  remove_extra_whitespaces: 1\n  escape_whitespaces: 1\n  normalization_rule_tsv: \n}\ndenormalizer_spec {}\ntrainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\ntrainer_interface.cc(185) LOG(INFO) Loading corpus: /kaggle/working/train_text_v2000.en\ntrainer_interface.cc(409) LOG(INFO) Loaded all 11905 sentences\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <pad>\ntrainer_interface.cc(430) LOG(INFO) Normalizing sentences...\ntrainer_interface.cc(539) LOG(INFO) all chars count=345766\ntrainer_interface.cc(560) LOG(INFO) Alphabet size=51\ntrainer_interface.cc(561) LOG(INFO) Final character coverage=1\ntrainer_interface.cc(592) LOG(INFO) Done! preprocessed 11905 sentences.\nunigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\nunigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=197786\nunigram_model_trainer.cc(312) LOG(INFO) Initialized 12478 seed sentencepieces\ntrainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 11905\ntrainer_interface.cc(609) LOG(INFO) Done! 7186\nunigram_model_trainer.cc(602) LOG(INFO) Using 7186 sentences for EM training\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=4689 obj=10.1461 num_tokens=14446 num_tokens/piece=3.08083\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=3889 obj=8.12357 num_tokens=14579 num_tokens/piece=3.74878\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=2915 obj=8.1278 num_tokens=15727 num_tokens/piece=5.3952\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=2913 obj=8.09174 num_tokens=15727 num_tokens/piece=5.3989\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=2200 obj=8.28584 num_tokens=17688 num_tokens/piece=8.04\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=2200 obj=8.23703 num_tokens=17688 num_tokens/piece=8.04\ntrainer_interface.cc(687) LOG(INFO) Saving model: /kaggle/working/en_spm_v2000.model\ntrainer_interface.cc(699) LOG(INFO) Saving vocabs: /kaggle/working/en_spm_v2000.vocab\nsentencepiece_trainer.cc(178) LOG(INFO) Running command: --input=/kaggle/working/train_text_v2000.id --model_prefix=/kaggle/working/id_spm_v2000 --vocab_size=2000 --character_coverage=1.0 --bos_id=1 --eos_id=2 --pad_id=3 --unk_id=0 --hard_vocab_limit=false\nsentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \ntrainer_spec {\n  input: /kaggle/working/train_text_v2000.id\n  input_format: \n  model_prefix: /kaggle/working/id_spm_v2000\n  model_type: UNIGRAM\n  vocab_size: 2000\n  self_test_sample_size: 0\n  character_coverage: 1\n  input_sentence_size: 0\n  shuffle_input_sentence: 1\n  seed_sentencepiece_size: 1000000\n  shrinking_factor: 0.75\n  max_sentence_length: 4192\n  num_threads: 16\n  num_sub_iterations: 2\n  max_sentencepiece_length: 16\n  split_by_unicode_script: 1\n  split_by_number: 1\n  split_by_whitespace: 1\n  split_digits: 0\n  pretokenization_delimiter: \n  treat_whitespace_as_suffix: 0\n  allow_whitespace_only_pieces: 0\n  required_chars: \n  byte_fallback: 0\n  vocabulary_output_piece_score: 1\n  train_extremely_large_corpus: 0\n  seed_sentencepieces_file: \n  hard_vocab_limit: 0\n  use_all_vocab: 0\n  unk_id: 0\n  bos_id: 1\n  eos_id: 2\n  pad_id: 3\n  unk_piece: <unk>\n  bos_piece: <s>\n  eos_piece: </s>\n  pad_piece: <pad>\n  unk_surface:  ⁇ \n  enable_differential_privacy: 0\n  differential_privacy_noise_level: 0\n  differential_privacy_clipping_threshold: 0\n}\nnormalizer_spec {\n  name: nmt_nfkc\n  add_dummy_prefix: 1\n  remove_extra_whitespaces: 1\n  escape_whitespaces: 1\n  normalization_rule_tsv: \n}\ndenormalizer_spec {}\ntrainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\ntrainer_interface.cc(185) LOG(INFO) Loading corpus: /kaggle/working/train_text_v2000.id\ntrainer_interface.cc(409) LOG(INFO) Loaded all 11905 sentences\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <pad>\ntrainer_interface.cc(430) LOG(INFO) Normalizing sentences...\ntrainer_interface.cc(539) LOG(INFO) all chars count=390166\ntrainer_interface.cc(560) LOG(INFO) Alphabet size=54\ntrainer_interface.cc(561) LOG(INFO) Final character coverage=1\ntrainer_interface.cc(592) LOG(INFO) Done! preprocessed 11905 sentences.\nunigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\nunigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=229298\nunigram_model_trainer.cc(312) LOG(INFO) Initialized 15128 seed sentencepieces\ntrainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 11905\ntrainer_interface.cc(609) LOG(INFO) Done! 7992\nunigram_model_trainer.cc(602) LOG(INFO) Using 7992 sentences for EM training\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=5381 obj=11.0306 num_tokens=16606 num_tokens/piece=3.08604\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=4555 obj=8.62093 num_tokens=16691 num_tokens/piece=3.66432\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=3414 obj=8.62797 num_tokens=17799 num_tokens/piece=5.21353\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=3410 obj=8.56943 num_tokens=17800 num_tokens/piece=5.21994\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=2557 obj=8.78648 num_tokens=19836 num_tokens/piece=7.75753\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=2557 obj=8.72262 num_tokens=19837 num_tokens/piece=7.75792\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=2200 obj=8.85698 num_tokens=20959 num_tokens/piece=9.52682\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=2200 obj=8.82399 num_tokens=20960 num_tokens/piece=9.52727\ntrainer_interface.cc(687) LOG(INFO) Saving model: /kaggle/working/id_spm_v2000.model\ntrainer_interface.cc(699) LOG(INFO) Saving vocabs: /kaggle/working/id_spm_v2000.vocab\n","output_type":"stream"},{"name":"stdout","text":"[SPM] Model saved: /kaggle/working/en_spm_v2000.model and /kaggle/working/id_spm_v2000.model\n[SPM] EN vocab: 2000 | ID vocab: 2000\n[2/6] Membuat datasets & loaders…\n[3/6] Melatih Transformer (vocab=2000)…\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Transformer Training:   0%|          | 0/373 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6743fe21f08141f6aaffb3ac00dc8b0a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/1488 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"78bd83ee6bd64b77b01408812ea9ed59"}},"metadata":{}},{"name":"stdout","text":"[Transformer][Epoch  1] loss=2.3012 | valBLEU=0.00 | valchrF=36.82\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Transformer Training:   0%|          | 0/373 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cb2acc64782b44719fca88b22ea9dbde"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/1488 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"08713120dc62494ab0f84e5841b7cafb"}},"metadata":{}},{"name":"stdout","text":"[Transformer][Epoch  2] loss=1.9379 | valBLEU=0.00 | valchrF=38.82\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Transformer Training:   0%|          | 0/373 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bcb68ccce8cd4a1dadbe7ac8751dfe3f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/1488 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"68169ea01b834e78bad78c8769cd4df0"}},"metadata":{}},{"name":"stdout","text":"[Transformer][Epoch  3] loss=1.8199 | valBLEU=0.00 | valchrF=38.82\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Transformer Training:   0%|          | 0/373 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"482d3910c1f94370b12e20bcd45c2c0d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/1488 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dedeb3c4664c4b1bad4b7da5954fe13d"}},"metadata":{}},{"name":"stdout","text":"[Transformer][Epoch  4] loss=1.7246 | valBLEU=35.36 | valchrF=41.38\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Transformer Training:   0%|          | 0/373 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a0b4b96e6a0a4a0aa5a5adf561fe312a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/1488 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a3fd3c29fe3a4c28ba190d51ad3425c6"}},"metadata":{}},{"name":"stdout","text":"[Transformer][Epoch  5] loss=1.6709 | valBLEU=0.00 | valchrF=35.41\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Transformer Training:   0%|          | 0/373 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a169e4da375d495b8e61108cbf0e4c27"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/1488 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e97a05fcd14b49f3936225c7dacc386b"}},"metadata":{}},{"name":"stdout","text":"[Transformer][Epoch  6] loss=1.6117 | valBLEU=0.00 | valchrF=35.41\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Transformer Training:   0%|          | 0/373 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc8bf8d5e273409c9c7fbee79a96991c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/1488 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee0472cb7a9841c79a0158f8c34d0d69"}},"metadata":{}},{"name":"stdout","text":"[Transformer][Epoch  7] loss=1.5696 | valBLEU=0.00 | valchrF=37.00\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Transformer Training:   0%|          | 0/373 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf11e69eb1bc45b99ea07fdbae474498"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/1488 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e0331a10c08e40f897c2e97472a7e4b9"}},"metadata":{}},{"name":"stdout","text":"[Transformer][Epoch  8] loss=1.5357 | valBLEU=35.36 | valchrF=47.95\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Transformer Training:   0%|          | 0/373 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fb832b61fb51438599528c3368af14d4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/1488 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d14abeeb19114179b42236e7f30b6b68"}},"metadata":{}},{"name":"stdout","text":"[Transformer][Epoch  9] loss=1.4968 | valBLEU=35.36 | valchrF=47.95\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Transformer Training:   0%|          | 0/373 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"764acfca392c434987092e93ad44234f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/1488 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b75d6a7c7bde4f1ca325dc1666a783d7"}},"metadata":{}},{"name":"stdout","text":"[Transformer][Epoch 10] loss=1.4828 | valBLEU=35.36 | valchrF=47.95\n[4/6] Melatih RNN+Attention (vocab=2000)…\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"RNN+Attn Training:   0%|          | 0/373 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c7db68dc6264d4dabcc0b70882de2c9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/1488 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7a248f14774b49deaf93db4d5f6c198b"}},"metadata":{}},{"name":"stdout","text":"[RNN+Attn][Epoch  1] loss=4.9569 | valBLEU=50.00 | valchrF=55.73\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"RNN+Attn Training:   0%|          | 0/373 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e2c509efbb2f4c5787c707380c0088d6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/1488 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b75b95a68cf54a19842862a4745e5452"}},"metadata":{}},{"name":"stdout","text":"[RNN+Attn][Epoch  2] loss=4.0627 | valBLEU=35.36 | valchrF=38.57\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"RNN+Attn Training:   0%|          | 0/373 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"49383297425e44f894e950c8b800c38a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/1488 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"951d361e459d413896bc6ae1fedb9928"}},"metadata":{}},{"name":"stdout","text":"[RNN+Attn][Epoch  3] loss=3.5473 | valBLEU=35.36 | valchrF=47.49\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"RNN+Attn Training:   0%|          | 0/373 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a1101b5652924e5a8b1c79114b9049d2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/1488 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d8e346ba63004faf8e5c517f3903c6ae"}},"metadata":{}},{"name":"stdout","text":"[RNN+Attn][Epoch  4] loss=3.1416 | valBLEU=35.36 | valchrF=68.24\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"RNN+Attn Training:   0%|          | 0/373 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a466675dfff44e1e8fb85c79f3aac539"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/1488 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8fad7646ed0d4282955c7519dc979cc0"}},"metadata":{}},{"name":"stdout","text":"[RNN+Attn][Epoch  5] loss=2.8380 | valBLEU=35.36 | valchrF=68.24\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"RNN+Attn Training:   0%|          | 0/373 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"df70b4c3c86b4615b7353a861468c939"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/1488 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b88bfab9bf6243a9b2ee6a6cf8212516"}},"metadata":{}},{"name":"stdout","text":"[RNN+Attn][Epoch  6] loss=2.6888 | valBLEU=50.00 | valchrF=70.84\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"RNN+Attn Training:   0%|          | 0/373 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"84f61a0754714a3ebedb10fcd9bf6d4e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/1488 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"65eb6a3a89834136972bfa448b344e8f"}},"metadata":{}},{"name":"stdout","text":"[RNN+Attn][Epoch  7] loss=2.5558 | valBLEU=35.36 | valchrF=68.24\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"RNN+Attn Training:   0%|          | 0/373 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec62a5d475fb47cb813f7938edd1b985"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/1488 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"91903fe959584f9f9184b953f252d029"}},"metadata":{}},{"name":"stdout","text":"[RNN+Attn][Epoch  8] loss=2.4568 | valBLEU=35.36 | valchrF=68.24\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"RNN+Attn Training:   0%|          | 0/373 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f8c3b9d7517470595315983d4d21608"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/1488 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"67d2dd64617b4b85904e2a18898f2838"}},"metadata":{}},{"name":"stdout","text":"[RNN+Attn][Epoch  9] loss=2.3975 | valBLEU=35.36 | valchrF=68.24\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"RNN+Attn Training:   0%|          | 0/373 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"42a9d5590ac34fec84758c2138c8f146"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/1488 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1cc96d3fa9674a10b8e2604950061693"}},"metadata":{}},{"name":"stdout","text":"[RNN+Attn][Epoch 10] loss=2.3399 | valBLEU=35.36 | valchrF=68.24\n[5/6] Evaluasi akhir (vocab=2000)…\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/1488 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f0fb93ed37146279123a0b2baa93a97"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/1488 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c7bdf5e6e4b0402b8ec089e215c9b315"}},"metadata":{}},{"name":"stdout","text":"\n[FINAL] Transformer : BLEU=53.73 | chrF=53.22\n[FINAL] RNN+Attention: BLEU=100.00 | chrF=100.00\n[6/6] Menyimpan hasil (vocab=2000)…\n\n[VOCAB=2000] Pelatihan selesai dalam 47.7 menit\n\n============================================================\nEXPERIMENT: VOCAB_SIZE = 4000\n============================================================\n[1/6] Membuat tokenizer…\n[SPM] Model saved: /kaggle/working/en_spm_v4000.model and /kaggle/working/id_spm_v4000.model\n[SPM] EN vocab: 3898 | ID vocab: 4000\n[2/6] Membuat datasets & loaders…\n[3/6] Melatih Transformer (vocab=4000)…\n","output_type":"stream"},{"name":"stderr","text":"sentencepiece_trainer.cc(178) LOG(INFO) Running command: --input=/kaggle/working/train_text_v4000.en --model_prefix=/kaggle/working/en_spm_v4000 --vocab_size=4000 --character_coverage=1.0 --bos_id=1 --eos_id=2 --pad_id=3 --unk_id=0 --hard_vocab_limit=false\nsentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \ntrainer_spec {\n  input: /kaggle/working/train_text_v4000.en\n  input_format: \n  model_prefix: /kaggle/working/en_spm_v4000\n  model_type: UNIGRAM\n  vocab_size: 4000\n  self_test_sample_size: 0\n  character_coverage: 1\n  input_sentence_size: 0\n  shuffle_input_sentence: 1\n  seed_sentencepiece_size: 1000000\n  shrinking_factor: 0.75\n  max_sentence_length: 4192\n  num_threads: 16\n  num_sub_iterations: 2\n  max_sentencepiece_length: 16\n  split_by_unicode_script: 1\n  split_by_number: 1\n  split_by_whitespace: 1\n  split_digits: 0\n  pretokenization_delimiter: \n  treat_whitespace_as_suffix: 0\n  allow_whitespace_only_pieces: 0\n  required_chars: \n  byte_fallback: 0\n  vocabulary_output_piece_score: 1\n  train_extremely_large_corpus: 0\n  seed_sentencepieces_file: \n  hard_vocab_limit: 0\n  use_all_vocab: 0\n  unk_id: 0\n  bos_id: 1\n  eos_id: 2\n  pad_id: 3\n  unk_piece: <unk>\n  bos_piece: <s>\n  eos_piece: </s>\n  pad_piece: <pad>\n  unk_surface:  ⁇ \n  enable_differential_privacy: 0\n  differential_privacy_noise_level: 0\n  differential_privacy_clipping_threshold: 0\n}\nnormalizer_spec {\n  name: nmt_nfkc\n  add_dummy_prefix: 1\n  remove_extra_whitespaces: 1\n  escape_whitespaces: 1\n  normalization_rule_tsv: \n}\ndenormalizer_spec {}\ntrainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\ntrainer_interface.cc(185) LOG(INFO) Loading corpus: /kaggle/working/train_text_v4000.en\ntrainer_interface.cc(409) LOG(INFO) Loaded all 11905 sentences\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <pad>\ntrainer_interface.cc(430) LOG(INFO) Normalizing sentences...\ntrainer_interface.cc(539) LOG(INFO) all chars count=345766\ntrainer_interface.cc(560) LOG(INFO) Alphabet size=51\ntrainer_interface.cc(561) LOG(INFO) Final character coverage=1\ntrainer_interface.cc(592) LOG(INFO) Done! preprocessed 11905 sentences.\nunigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\nunigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=197786\nunigram_model_trainer.cc(312) LOG(INFO) Initialized 12478 seed sentencepieces\ntrainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 11905\ntrainer_interface.cc(609) LOG(INFO) Done! 7186\nunigram_model_trainer.cc(602) LOG(INFO) Using 7186 sentences for EM training\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=4689 obj=10.1461 num_tokens=14446 num_tokens/piece=3.08083\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=3889 obj=8.12357 num_tokens=14579 num_tokens/piece=3.74878\ntrainer_interface.cc(687) LOG(INFO) Saving model: /kaggle/working/en_spm_v4000.model\ntrainer_interface.cc(699) LOG(INFO) Saving vocabs: /kaggle/working/en_spm_v4000.vocab\nsentencepiece_trainer.cc(178) LOG(INFO) Running command: --input=/kaggle/working/train_text_v4000.id --model_prefix=/kaggle/working/id_spm_v4000 --vocab_size=4000 --character_coverage=1.0 --bos_id=1 --eos_id=2 --pad_id=3 --unk_id=0 --hard_vocab_limit=false\nsentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \ntrainer_spec {\n  input: /kaggle/working/train_text_v4000.id\n  input_format: \n  model_prefix: /kaggle/working/id_spm_v4000\n  model_type: UNIGRAM\n  vocab_size: 4000\n  self_test_sample_size: 0\n  character_coverage: 1\n  input_sentence_size: 0\n  shuffle_input_sentence: 1\n  seed_sentencepiece_size: 1000000\n  shrinking_factor: 0.75\n  max_sentence_length: 4192\n  num_threads: 16\n  num_sub_iterations: 2\n  max_sentencepiece_length: 16\n  split_by_unicode_script: 1\n  split_by_number: 1\n  split_by_whitespace: 1\n  split_digits: 0\n  pretokenization_delimiter: \n  treat_whitespace_as_suffix: 0\n  allow_whitespace_only_pieces: 0\n  required_chars: \n  byte_fallback: 0\n  vocabulary_output_piece_score: 1\n  train_extremely_large_corpus: 0\n  seed_sentencepieces_file: \n  hard_vocab_limit: 0\n  use_all_vocab: 0\n  unk_id: 0\n  bos_id: 1\n  eos_id: 2\n  pad_id: 3\n  unk_piece: <unk>\n  bos_piece: <s>\n  eos_piece: </s>\n  pad_piece: <pad>\n  unk_surface:  ⁇ \n  enable_differential_privacy: 0\n  differential_privacy_noise_level: 0\n  differential_privacy_clipping_threshold: 0\n}\nnormalizer_spec {\n  name: nmt_nfkc\n  add_dummy_prefix: 1\n  remove_extra_whitespaces: 1\n  escape_whitespaces: 1\n  normalization_rule_tsv: \n}\ndenormalizer_spec {}\ntrainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\ntrainer_interface.cc(185) LOG(INFO) Loading corpus: /kaggle/working/train_text_v4000.id\ntrainer_interface.cc(409) LOG(INFO) Loaded all 11905 sentences\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <pad>\ntrainer_interface.cc(430) LOG(INFO) Normalizing sentences...\ntrainer_interface.cc(539) LOG(INFO) all chars count=390166\ntrainer_interface.cc(560) LOG(INFO) Alphabet size=54\ntrainer_interface.cc(561) LOG(INFO) Final character coverage=1\ntrainer_interface.cc(592) LOG(INFO) Done! preprocessed 11905 sentences.\nunigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\nunigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=229298\nunigram_model_trainer.cc(312) LOG(INFO) Initialized 15128 seed sentencepieces\ntrainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 11905\ntrainer_interface.cc(609) LOG(INFO) Done! 7992\nunigram_model_trainer.cc(602) LOG(INFO) Using 7992 sentences for EM training\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=5381 obj=11.0306 num_tokens=16606 num_tokens/piece=3.08604\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=4555 obj=8.62093 num_tokens=16691 num_tokens/piece=3.66432\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=4277 obj=8.54166 num_tokens=16711 num_tokens/piece=3.90718\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=4242 obj=8.51998 num_tokens=16730 num_tokens/piece=3.94389\ntrainer_interface.cc(687) LOG(INFO) Saving model: /kaggle/working/id_spm_v4000.model\ntrainer_interface.cc(699) LOG(INFO) Saving vocabs: /kaggle/working/id_spm_v4000.vocab\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Transformer Training:   0%|          | 0/373 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c1b813eb0f794fdd8715391fa8d4c098"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/1488 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e312396a5d2f49059501adcaa477fd3f"}},"metadata":{}},{"name":"stdout","text":"[Transformer][Epoch  1] loss=2.5601 | valBLEU=0.00 | valchrF=15.15\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Transformer Training:   0%|          | 0/373 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4f08c89799704d07ba49d8a4db22a178"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/1488 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9a8ad2c819b4480cb80859be52b31596"}},"metadata":{}},{"name":"stdout","text":"[Transformer][Epoch  2] loss=2.0579 | valBLEU=0.00 | valchrF=51.14\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Transformer Training:   0%|          | 0/373 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eeb0a6418fc04fc8a7b234a05b883a66"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/1488 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"15767dc24d3848948e936d0a1114dd9f"}},"metadata":{}},{"name":"stdout","text":"[Transformer][Epoch  3] loss=1.9509 | valBLEU=0.00 | valchrF=38.82\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Transformer Training:   0%|          | 0/373 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c0873b2134764ed0aea029e3d6eecdeb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/1488 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3e57818d9f40446f8e73b2963cef2966"}},"metadata":{}},{"name":"stdout","text":"[Transformer][Epoch  4] loss=1.8627 | valBLEU=0.00 | valchrF=35.41\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Transformer Training:   0%|          | 0/373 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"529e9e0a1b7f49b2be481d3718f13a83"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/1488 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4d15d3e347db437bb9a9a9913fd7f35c"}},"metadata":{}},{"name":"stdout","text":"[Transformer][Epoch  5] loss=1.7882 | valBLEU=45.18 | valchrF=41.97\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Transformer Training:   0%|          | 0/373 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5a3b6498e3464eb985d20f8fdfab1d1e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/1488 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a60777a2a0d846589bd94202de77bc5a"}},"metadata":{}},{"name":"stdout","text":"[Transformer][Epoch  6] loss=1.7652 | valBLEU=45.18 | valchrF=41.97\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Transformer Training:   0%|          | 0/373 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b80c569d7c6d4cbea03f0d6d6e9973dc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/1488 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"280dc18ea51d4da388adccd17f0e1941"}},"metadata":{}},{"name":"stdout","text":"[Transformer][Epoch  7] loss=1.7363 | valBLEU=0.00 | valchrF=35.41\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Transformer Training:   0%|          | 0/373 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e9b0d5f9bb94223898223f92122d4d0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/1488 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1960852d23174656ac7e0b6c18ff9ce9"}},"metadata":{}},{"name":"stdout","text":"[Transformer][Epoch  8] loss=1.6910 | valBLEU=50.00 | valchrF=40.82\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Transformer Training:   0%|          | 0/373 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c1a4757c7e704b59a3f9a113557666ec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/1488 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bd5cb86ee8634ab887744a47e6df0f44"}},"metadata":{}},{"name":"stdout","text":"[Transformer][Epoch  9] loss=1.6816 | valBLEU=50.00 | valchrF=53.25\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Transformer Training:   0%|          | 0/373 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f90c2d7db16b425db3a20bca97408a19"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/1488 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"96347952a47945cbb244c6d9632fb210"}},"metadata":{}},{"name":"stdout","text":"[Transformer][Epoch 10] loss=1.6494 | valBLEU=45.18 | valchrF=42.30\n[4/6] Melatih RNN+Attention (vocab=4000)…\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"RNN+Attn Training:   0%|          | 0/373 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"583f833feda94ee8bb71604dd623885b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/1488 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"18ac9c83d8e14ebd9c9e88dbb85ec73c"}},"metadata":{}},{"name":"stdout","text":"[RNN+Attn][Epoch  1] loss=5.1155 | valBLEU=45.18 | valchrF=56.71\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"RNN+Attn Training:   0%|          | 0/373 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e7cded37c73847d0a92a67466add8684"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/1488 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ffb18bf255d4473fa0bd2874ac3dc724"}},"metadata":{}},{"name":"stdout","text":"[RNN+Attn][Epoch  2] loss=4.1561 | valBLEU=74.01 | valchrF=73.54\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"RNN+Attn Training:   0%|          | 0/373 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"204ee445347e438e8221d53699f26f7f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/1488 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"caaa2d3aa6de404da6316d358a696d46"}},"metadata":{}},{"name":"stdout","text":"[RNN+Attn][Epoch  3] loss=3.6704 | valBLEU=53.73 | valchrF=49.00\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"RNN+Attn Training:   0%|          | 0/373 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1c5950c9ff4c44adb2912742bd843924"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/1488 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4928fdff4a1b4891a6df32edc70f0740"}},"metadata":{}},{"name":"stdout","text":"[RNN+Attn][Epoch  4] loss=3.2208 | valBLEU=45.18 | valchrF=73.65\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"RNN+Attn Training:   0%|          | 0/373 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7b81574a212740f4be17b5f8546baba8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/1488 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d7e484bb32454abaada69c8e35a93e3e"}},"metadata":{}},{"name":"stdout","text":"[RNN+Attn][Epoch  5] loss=2.8427 | valBLEU=45.18 | valchrF=73.65\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"RNN+Attn Training:   0%|          | 0/373 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0b7fac660362465884d9f01f34c3b065"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/1488 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c74b1379d6d14dc8ac82f0feb4c4b32d"}},"metadata":{}},{"name":"stdout","text":"[RNN+Attn][Epoch  6] loss=2.5502 | valBLEU=45.18 | valchrF=73.65\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"RNN+Attn Training:   0%|          | 0/373 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"15cc7a6089474a4fbdf62228ba388d68"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/1488 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"747c24519f57402f97c89bb5ff98b30d"}},"metadata":{}},{"name":"stdout","text":"[RNN+Attn][Epoch  7] loss=2.4118 | valBLEU=45.18 | valchrF=73.65\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"RNN+Attn Training:   0%|          | 0/373 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1d26a0ec80b14629b4afb1c8b40b65c4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/1488 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"17b4312a9f8642cb83ecd953105d95d8"}},"metadata":{}},{"name":"stdout","text":"[RNN+Attn][Epoch  8] loss=2.2815 | valBLEU=45.18 | valchrF=73.65\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"RNN+Attn Training:   0%|          | 0/373 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0d0743f3a2f7437ea086fd8fe993c541"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/1488 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd755b77ba024fe1b0c41374c82a99ad"}},"metadata":{}},{"name":"stdout","text":"[RNN+Attn][Epoch  9] loss=2.1691 | valBLEU=35.36 | valchrF=65.26\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"RNN+Attn Training:   0%|          | 0/373 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a620fd766c3a46ba8f2f33fd733a0f68"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/1488 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf26f8a45d374ba98fd1e96ac3159855"}},"metadata":{}},{"name":"stdout","text":"[RNN+Attn][Epoch 10] loss=2.1095 | valBLEU=35.36 | valchrF=65.26\n[5/6] Evaluasi akhir (vocab=4000)…\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/1488 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7531d4db370e4d9982b91b5925799666"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/1488 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c76e761b07594b80b67b17dc5ef52e8b"}},"metadata":{}},{"name":"stdout","text":"\n[FINAL] Transformer : BLEU=35.36 | chrF=47.96\n[FINAL] RNN+Attention: BLEU=90.36 | chrF=86.59\n[6/6] Menyimpan hasil (vocab=4000)…\n\n[VOCAB=4000] Pelatihan selesai dalam 48.8 menit\n\n================================================================================\nRINGKASAN STUDI ABLASI\n================================================================================\n\nHASIL ABLASI:\n vocab_size  tr_bleu  tr_chrf  rnn_bleu  rnn_chrf  training_time_min\n       2000    53.73    53.22    100.00    100.00              47.66\n       4000    35.36    47.96     90.36     86.59              48.78\n\nKONFIGURASI TERBAIK:\nTransformer: vocab=2000, BLEU=53.73\nRNN+Attn : vocab=2000, BLEU=100.00\n\n================================================================================\nCONTOH TERJEMAHAN DARI HASIL TERBAIK\n================================================================================\n\n--- Prediksi Transformer ---\n                                       src                  hyp  \\\n0           tom is going to do that again.  tom tidak tahu itu.   \n1                          there is a cat.      adalah di sini.   \n2                     this is interesting.          ini adalah.   \n3  how many countries are there in europe?      apakah di mana?   \n4                        i can't eat pork.      aku tidak bisa.   \n\n                                   ref  \n0         tom akan melakukan itu lagi.  \n1                   ada seekor kucing.  \n2                         ini menarik.  \n3          ada berapa negara di eropa?  \n4  saya tidak boleh makan daging babi.  \n\n--- Prediksi RNN+Attention ---\n                                       src                              hyp  \\\n0           tom is going to do that again.     tom akan melakukan itu lagi.   \n1                          there is a cat.                      ada kucing.   \n2                     this is interesting.                             ini.   \n3  how many countries are there in europe?  berapa banyak yang ada di sana?   \n4                        i can't eat pork.            aku tidak bisa makan.   \n\n                                   ref  \n0         tom akan melakukan itu lagi.  \n1                   ada seekor kucing.  \n2                         ini menarik.  \n3          ada berapa negara di eropa?  \n4  saya tidak boleh makan daging babi.  \n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"Penjelasan:\n\nkode ini adalah bagian eksekusi utama. Bagian ini akan:\n\n- Memanggil fungsi run_ablation_study untuk menjalankan eksperimen pada ukuran vocabulary yang berbeda.\n\n- Menggunakan create_ablation_summary untuk mengumpulkan hasil dari setiap eksperimen.\n\n- Mencetak tabel ringkasan, membuat grafik perbandingan, dan menampilkan contoh hasil terjemahan terbaik.","metadata":{}}]}